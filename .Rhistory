# stem
tweets.corpus <- tm_map(tweets.corpus, stemDocument)
inspect(tweets.corpus[4])
# stem completition
tweets.corpus <- tm_map(tweets.corpus, stemCompletion,
dictionary=tweets.corpus.copy)
save(tweets.corpus, file="tweets.corpus.SC.RData")
# to TD matrix ####
library(tm)
# tdm
tweets.tdm <- TermDocumentMatrix(tweets.corpus, control=list(wordLengths=c(1, Inf)))
# print terms
dimnames(tweets.tdm)$Terms
save(tweets.tdm, file="tweets.tdm.RData")
# frequent terms
which(apply(tweets.tdm, 1, sum) > 20)
findFreqTerms(tweets.tdm, lowfreq=20)
findAssocs(tweets.tdm, "mandela", 0.25)
findAssocs(tweets.tdm, "recent", 0.25)
# word cloud ####
library(wordcloud)
tweets.matrix <- as.matrix(tweets.tdm)
wordFreq.sort <- sort(rowSums(tweets.matrix), decreasing=T)
save(tweets.matrix, file="tweets.matrix.RData")
# wcloud
set.seed(1234)
grayLevels <- gray( (wordFreq.sort + 10) / (max(wordFreq.sort) + 10))
word.cloud <- wordcloud(words=names(wordFreq.sort), freq=wordFreq.sort,
min.freq=3, random.order=F, colors=grayLevels)
# tweetsomist, weeks, america, new, year, china, mandela, december
#\-------------hclust-----------\####
# hierarchial clustering of terms ####
setwd("~/Documents/R/mining/twitter")
load("tweets.tdm.RData")
# remove sparse terms ####
tweets.tdm2 <- removeSparseTerms(tweets.tdm, sparse=0.95)
tweets.matrix2 <- as.matrix(tweets.tdm2)
# hclust ####
distMatrix <- dist(scale(tweets.matrix2))
tweets.fit <- hclust(distMatrix, method="ward")
# plot dendrogram ####
plot(tweets.fit, cex=0.9, hang=-1,
main="Word Cluster Dendrogram")
# cut tree
rect.hclust(tweets.fit, k=5)
(tweets.groups <- cutree(tweets.fit, k=5))
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/Desktop/R/Text_Analysis/shiny/TermFreq2/app.R')
.rs.enableRStudioConnectUI(TRUE)
source('~/Desktop/R/RTwitter/hashtagTwitter.R')
source('~/Desktop/R/hashtagTwitter.R')
tweetlist <- sapply(tweets, function(x) x$text)
#Strip URLS
tweetlist=rm_url(tweetlist, extract = FALSE)
#strip @mentions and @usernames
tweetlist=gsub("@(.*)", "", tweetlist)
#Strip punctuation
tweetlist=gsub( "[^[:alnum:] ]", "", tweetlist )
#Split into words
words <-strsplit(tweetlist, "\\W+", perl=TRUE)
# #Remove common words
words=rm_stopwords(words,c(Top200Words, "twitter", "rt", "amp", "youre", "heres", "retweeted"))
#Get rid of empty elements
words=words[lapply(words,length)>0]
#Flatten the list of lists
words=unlist(words,recursive = FALSE)
#Convert to a sorted frequency table
words=sort(table(words),decreasing=T)
freqs=as.vector(words)
words=names(words)
cols <- colorRampPalette(brewer.pal(12,"Paired"))(500)
wordcloud(words,freqs,scale=c(3,1),min.freq=3,max.words=100, colors=brewer.pal(8, "Dark2"))
source('~/Desktop/R/hashtagTwitter.R')
wordcloud(words, freqs, random.order=FALSE,scale=c(4,1),rot.per=0,
max.words=100,colors=brewer.pal(8, "Dark2"))
wordcloud(words, freqs, random.order=FALSE,scale=c(3,1),rot.per=0,
max.words=100,colors=brewer.pal(8, "Dark2"))
source('~/Desktop/R/hashtagTwitter.R')
source('~/Desktop/R/hashtagTwitter.R')
library(NLP)
library(tm)
library(SnowballC)
library(cluster)
#Set the working directory
#setwd("~/Text-Analysis/")
#Create a corpus
corpus <- Corpus(DirSource("data/shakespeareFolger"))
setwd("~/Desktop/R/Text_AnalysisNotGIT")
library(NLP)
library(tm)
library(SnowballC)
library(cluster)
#Set the working directory
#setwd("~/Text-Analysis/")
#Create a corpus
corpus <- Corpus(DirSource("data/shakespeareFolger"))
as.character(corpus[1])
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
library(NLP)
library(tm)
library(SnowballC)
library(cluster)
#Set the working directory
#setwd("~/Text-Analysis/")
#Create a corpus
corpus <- Corpus(DirSource("data/shakespeareFolger"))
corpus <- tm_map(corpus,
content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')),
mc.cores=1)
#Clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
#To change the stopword list, use other dictionaries available with the tm package
#Add early modern stopwords
myStopWords <- scan("data/earlyModernStopword.txt", what="character", sep="\n")
corpus <- tm_map(corpus, removeWords, c(stopwords("english"), myStopWords))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpusCopy, stemDocument)
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, Inf)))
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
library(NLP)
library(tm)
library(SnowballC)
library(cluster)
#Set the working directory
#setwd("~/Text-Analysis/")
#Create a corpus
corpus <- Corpus(DirSource("data/shakespeareFolger"))
#corpus <- tm_map(corpus,
# content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')),
# mc.cores=1)
#Clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
#To change the stopword list, use other dictionaries available with the tm package
#Add early modern stopwords
myStopWords <- scan("data/earlyModernStopword.txt", what="character", sep="\n")
corpus <- tm_map(corpus, removeWords, c(stopwords("english"), myStopWords))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
dtm <- DocumentTermMatrix(corpus)
dtm$dimnames$Docs
library(NLP)
library(tm)
library(SnowballC)
library(cluster)
#Set the working directory
#setwd("~/Text-Analysis/")
#Create a corpus
corpus <- Corpus(DirSource("data/shakespeareFolger"))
#corpus <- tm_map(corpus,
# content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')),
# mc.cores=1)
#Clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
#To change the stopword list, use other dictionaries available with the tm package
#Add early modern stopwords
myStopWords <- scan("data/earlyModernStopword.txt", what="character", sep="\n")
corpus <- tm_map(corpus, removeWords, c(stopwords("english"), myStopWords))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
dtm <- DocumentTermMatrix(corpus)
dtm2 <- removeSparseTerms(dtm, sparse=.95)
dtm2
dtm <- weightTfIdf(dtm)
inspect(dtm[1:10, 5001:5010])
#Convert dtm to a matrix using k-means (Euclidean distance)
m <- as.matrix(dtm)
#normalize the vectors
norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)
d <- dist(m, method="cosine")
hc <- hclust(d, method="average")
plot(hc, labels = df$text)
d <- dist(m, method="cosine")
hc <- hclust(d, method="average")
plot(hc, hang=-1)
library(NLP)
library(tm)
library(SnowballC)
library(cluster)
#Set the working directory
#setwd("~/Text-Analysis/")
#Create a corpus
corpus <- Corpus(DirSource("data/shakespeareFolger"))
#corpus <- tm_map(corpus,
# content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')),
# mc.cores=1)
#Clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
#To change the stopword list, use other dictionaries available with the tm package
#Add early modern stopwords
myStopWords <- scan("data/earlyModernStopword.txt", what="character", sep="\n")
corpus <- tm_map(corpus, removeWords, c(stopwords("english"), myStopWords))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
dtm <- DocumentTermMatrix(corpus)
# Weight dtm by inverse frequency
dtm <- weightTfIdf(dtm)
#Convert dtm to a matrix using k-means (Euclidean distance)
m <- as.matrix(dtm)
smaller.m <- m[,apply(final.m,2,mean)>=.25]
smaller.m <- m[,apply(m,2,mean)>=.25]
smaller.m
d <- dist(smaller.m)
hc <- hclust(d)
plot(hc, hang=-1)
d <- dist(smaller.m)
hc <- hclust(d)
plot(hc)
source('~/.active-rstudio-document')
d <- dist(smaller.m)
hc <- hclust(d, method="Ward.d")
plot(hc)
d <- dist(smaller.m)
hc <- hclust(d, method="Ward")
plot(hc)
d <- dist(smaller.m)
hc <- hclust(d, method="ward.D")
plot(hc)
d <- dist(smaller.m)
hc <- hclust(d, method="ward.D")
plot(hc)
source('~/.active-rstudio-document')
d <- dist(smaller.m)
hc <- hclust(d, method="ward.D")
plot(hc)
d <- dist(smaller.m)
hc <- hclust(d)
plot(hc)
source('~/.active-rstudio-document')
library(NLP)
library(tm)
library(SnowballC)
library(cluster)
#Set the working directory
#setwd("~/Text-Analysis/")
#Create a corpus
corpus <- Corpus(DirSource("data/shakespeareFolger"))
#corpus <- tm_map(corpus,
# content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')),
# mc.cores=1)
#Clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
#To change the stopword list, use other dictionaries available with the tm package
#Add early modern stopwords
myStopWords <- scan("data/earlyModernStopword.txt", what="character", sep="\n")
corpus <- tm_map(corpus, removeWords, c(stopwords("english"), myStopWords))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
dtm <- DocumentTermMatrix(corpus)
# Weight dtm by inverse frequency
dtm <- weightTfIdf(dtm)
#Convert dtm to a matrix using k-means (Euclidean distance)
m <- as.matrix(dtm)
smaller.m <- m[,apply(m,2,mean)>=.25]
m
smaller.m
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
dtm
as.character(dtm[1])
inspect(dtm[1:10,1:10])
inspect(dtm[1:10])
inspect(dtm[1:10,1:10])
findFreqTerms(dtm, 2000)
source('~/Desktop/R/Text_AnalysisNotGIT/RScripts/cooccurrencePlainText.R')
findFreqTerms(tdm, 2000)
findAssocs(tdm, "love", 0.8)
inspect(dtm)
findAssocs(tdm, "love", 0.8)
source('~/.active-rstudio-document')
findAssocs(tdm, "love", 0.8)
findAssocs(tdm, "love", 0.6)
source('~/.active-rstudio-document')
findAssocs(tdm, "love", 0.6)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
findAssocs(tdm, "love", 0.6)
source('~/.active-rstudio-document')
findAssocs(tdm, "love", 0.6)
source('~/Desktop/R/RAnalysisPrivate/rScripts/corpusCluster.R')
findAssocs(tdm, "love", 0.6)
library(NLP)
library(tm)
library(SnowballC)
library(cluster)
#Set the working directory
#setwd("~/Text-Analysis/")
#Create a corpus
corpus <- Corpus(DirSource("data/bronte"))
setwd("~/Desktop/R/Text_AnalysisNotGIT")
library(NLP)
library(tm)
library(SnowballC)
library(cluster)
#Set the working directory
#setwd("~/Text-Analysis/")
#Create a corpus
corpus <- Corpus(DirSource("data/bronte"))
inspect(corpus[5])
as.character(corpus[1])
source('~/.active-rstudio-document')
writeLines(as.character(corpus[1]))
library(NLP)
library(tm)
library(SnowballC)
library(cluster)
#Set the working directory
#setwd("~/Text-Analysis/")
#Create a corpus
corpus <- Corpus(DirSource("data/shakespeareFolger"))
as.character(corpus[1])
writeLines(as.character(corpus[1]))
#Clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
#To change the stopword list, use other dictionaries available with the tm package
#Add early modern stopwords
myStopWords <- scan("data/earlyModernStopword.txt", what="character", sep="\n")
corpus <- tm_map(corpus, removeWords, c(stopwords("english"), myStopWords))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
writeLines(as.character(corpus[1]))
library(NLP)
library(tm)
library(SnowballC)
library(cluster)
#Set the working directory
#setwd("~/Text-Analysis/")
#Create a corpus
corpus <- Corpus(DirSource("data/shakespeareFolger"))
writeLines(as.character(corpus[1]))
corpus <- tm_map(corpus, content_transformer(tolower))
writeLines(as.character(corpus[1]))
myStopWords <- scan("data/earlyModernStopword.txt", what="character", sep="\n")
corpus <- tm_map(corpus, removeWords, c(stopwords("english"), myStopWords))
writeLines(as.character(corpus[1]))
grep("onion", corpus, ignore.case = TRUE))
grep("onion", corpus, ignore.case = TRUE)
library(NLP)
library(tm)
library(SnowballC)
library(cluster)
#Set the working directory
#setwd("~/Text-Analysis/")
#Create a corpus
corpus <- Corpus(DirSource("data/shakespeareFolger"))
#corpus <- tm_map(corpus,
# content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')),
# mc.cores=1)
#Clean the corpus
corpus <- tm_map(corpus, content_transformer(tolower))
#To change the stopword list, use other dictionaries available with the tm package
#Add early modern stopwords
#myStopWords <- scan("data/earlyModernStopword.txt", what="character", sep="\n")
corpus <- tm_map(corpus, removeWords, stopwords("english"))
grep("onion", corpus, ignore.case = TRUE)
writeLines(as.character(corpus[1]))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
writeLines(as.character(corpus[1]))
dtm <- DocumentTermMatrix(corpus)
tdm <- TermDocumentMatrix(corpus)
findAssocs(tdm, 2000)
findAssocs(tdm, "love", 0.8)
findAssocs(tdm, "love", 0.6)
dtm
inspect(dtm)
tdm
source('~/.active-rstudio-document')
freq
findAssocs(dtm, "love", .6)
findAssocs(tdm, "love", .6)
source('~/.active-rstudio-document')
findAssocs(tdm, "love", .6)
source('~/.active-rstudio-document')
findAssocs(tdm, "love", .6)
source('~/.active-rstudio-document')
findAssocs(tdm, "love", .6)
findAssocs(dtm, "love", .6)
findFreqTerms(dtm, 20)
findFreqTerms(dtm, 200)
findFreqTerms(dtm, 2000)
source('~/.active-rstudio-document')
findAssocs(dtm, "love", .6)
findAssocs(tdm, "love", .6)
source('~/.active-rstudio-document')
findAssocs(tdm, "love", .6)
findAssocs(tdm, "beauty", .6)
source('~/.active-rstudio-document')
findAssocs(myDtm, "love", .6)
myDtm
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
findAssocs(dtm, "love", .6)
inspect(corpus[8])
as.character(corpus[8])
#Top Ten Word Co-occurrence, Curated Stopwords Removed
library(NLP)
library(tm)
require(quanteda)
#Set the working directory
setwd("~/Desktop/R/Text_AnalysisNotGIT/data")
#Create a corpus
corpus <- Corpus(DirSource("shakespeareFolger"))
as.character(corpus[8])
#Top Ten Word Co-occurrence, Curated Stopwords Removed
library(NLP)
library(tm)
require(quanteda)
#Set the working directory
setwd("~/Desktop/R/Text_AnalysisNotGIT/data")
txt <- system.file("texts", "txt", package = "tm")
corpus <- VCorpus(DirSource("shakespeareFolger", encoding = "UTF-8"),
+ readerControl = list(language = "eng")))
#Top Ten Word Co-occurrence, Curated Stopwords Removed
library(NLP)
library(tm)
require(quanteda)
#Set the working directory
setwd("~/Desktop/R/Text_AnalysisNotGIT/data")
txt <- system.file("texts", "txt", package = "tm")
corpus <- VCorpus(DirSource("shakespeareFolger", encoding = "UTF-8")))
#Top Ten Word Co-occurrence, Curated Stopwords Removed
library(NLP)
library(tm)
require(quanteda)
#Set the working directory
setwd("~/Desktop/R/Text_AnalysisNotGIT/data")
txt <- system.file("texts", "txt", package = "tm")
corpus <- VCorpus(DirSource("shakespeareFolger", encoding = "UTF-8"))
as.character(corpus[8])
txt <- system.file("shakespeareFolger", "txt", package = "tm")
corpus <- VCorpus(DirSource("txt", encoding = "UTF-8"))
txt <- system.file("shakespeareFolger", "txt", package = "tm")
corpus <- VCorpus(DirSource(txt, encoding = "UTF-8"))
#Top Ten Word Co-occurrence, Curated Stopwords Removed
library(NLP)
library(tm)
require(quanteda)
#Set the working directory
setwd("~/Desktop/R/Text_AnalysisNotGIT/data")
txt <- system.file("texts, "shakespeareFolger", package = "tm")
corpus <- VCorpus(DirSource(txt, encoding = "UTF-8"))
#Top Ten Word Co-occurrence, Curated Stopwords Removed
library(NLP)
library(tm)
require(quanteda)
#Set the working directory
setwd("~/Desktop/R/Text_AnalysisNotGIT/data")
#Create a corpus
corpus <- Corpus(DirSource("shakespeareFolger"))
#Clean the corpus
corpus <- tm_map(corpus, PlainTextDocument)
as.character(corpus[8])
#Create a corpus
(corpus <- Corpus(DirSource("shakespeareFolger"), encoding = "UTF-8"), +
readerControl = list(language = "eng")))
#Create a corpus
(corpus <- Corpus(DirSource("shakespeareFolger"), encoding = "UTF-8"), +
readerControl = list(language = "eng")
#Top Ten Word Co-occurrence, Curated Stopwords Removed
library(NLP)
library(tm)
require(quanteda)
#Set the working directory
setwd("~/Desktop/R/Text_AnalysisNotGIT/data")
#Create a corpus
corpus <- Corpus(DirSource("shakespeareFolger"), encoding = "UTF-8")
corpus <- (Corpus(DirSource("shakespeareFolger"), encoding = "UTF-8")
readerControl = list("eng")
#Top Ten Word Co-occurrence, Curated Stopwords Removed
library(NLP)
library(tm)
require(quanteda)
#Set the working directory
setwd("~/Desktop/R/Text_AnalysisNotGIT/data")
corpus  <-Corpus(DirSource("shakespeareFolger"), readerControl = list(language="eng"))
as.character(corpus[8])
inspect(corpus[8])
corpus  <-Corpus(DirSource("bronte"), readerControl = list(language="eng"))
corpus[[1]]$content
corpus  <-Corpus(DirSource("shakespeareFolger"), readerControl = list(language="eng"))
corpus[[1]]$content
corpus <- tm_map(corpus, content_transformer(tolower))
corpus[[1]]$content
myStopWords <- scan("~/Desktop/R/Text_AnalysisNotGit/data/earlyModernStopword.txt", what="character", sep="\n")
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus[[1]]$content
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus[[1]]$content
findAssocs(dtm, "love", .6)
findAssocs(dtm, "love", .8)
findAssocs(dtm, "love", .1)
findAssocs(dtm, "love", .5)
findAssocs(dtm, "love", .6)
source('~/.active-rstudio-document')
setwd("~/Desktop/Text-Analysis/")
source('~/.active-rstudio-document')
